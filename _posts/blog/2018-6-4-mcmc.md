---
layout: post
title: "Markov Chains ;]"
modified: 2018-06-14
categories: blog
excerpt:
tags: []
date: 2018-06-14
---

## Markov Chains
By Markov chains in this blog we refer to [discrete-time homogeneous Markov chains](https://en.wikipedia.org/wiki/Markov_chain#Discrete-time_Markov_chain).

**Discrete-time Markov chains** 
A discrete-time Markov chain is a sequence of random variables \\(\{X_1, X_2, X_3, ...\}\\) with the Markov property, namely that the probability of moving to the next state depends only on the present state and not on the previous states
\\[P(X_{n+1}|X_n,…,X1) = P(X_{n+1}|X_n).\\]

**Homogeneous chains**
A homogeneous Markov chain is one that does not evolve in time, that is, its transition probabilities are independent of the time step \\(n\\)
\\[P(X_{n+1}|X_n) \text{ is same } \forall n \leq 0.\\]

## Water Lily Example
To illustrate we'll take a look at the water lily example from the [Bayesian Methods for Machine Learning course](https://www.coursera.org/learn/bayesian-methods-in-machine-learning) on cousera.
[lily]
There a frog in a pond that jumps between two water lilies according to some probability. As shown in the graph, if the frog is at the left (L) lily at time \\(n\\) then it has 0.7 probability to jump to the right (R) lily and 0.3 probability to stay till the next round \\(n+1\\), if the frog is at the right lily then probability for staying and jumping are even (0.5).

This process can be modeled as a Markov chain, let \\(X_n\in\\{L, R\\}\\) be the state of the frog at time \\(n\\), we have the trainsition probabilities
\\[P(X_{n+1}=L|X_n=L)=0.3, P(X_{n+1}=R|X_n=L)=0.7\\]
\\[P(X_{n+1}=L|X_n=R)=0.5, P(X_{n+1}=R|X_n=R)=0.5.\\]

If we let the frog jumps as decribed for a long enough period, it turns out the probability of the state of the frog will coverge to a fixed distribution \\(\pi\\) regardless of the starting state, specifically \\(\pi(X=L)=0.44\\), \\(\pi(X=R)=0.56\\). It is an interesting property of Markov chains and we'll spend the rest of this blog to introduce relevant concepts and applications.

## Stationary Distribution
If a distribution \\(\pi\\) remains unchanged in the Markov chain it is called a stationary distribution, formally if satisfies the condition
\\[\pi(X=i)=\sum_j\pi(X=j)P(X=i|X=j).\\]

If there's a finite numebr of possible states in the sample space, this condition can be put in matrix form as
\\[\pi=\pi P\\]
where \\(P_{ij}=P(X=i|X=j)\\). It might look similar to the definition of eigenvalues and eigenvectors
\\[Av=\lambda v\\]
In fact \\(\pi\\) is an eigenvector of \\(P^T\\) with eigenvalue \\(\lambda=1\\)
\\[(P^T-I)\pi=0\\]
and \\(\pi\\) is a vector in null space of \\(P^T-I\\) by definition. So to find the sationary distribution of a finite state Markov chain, we only need to solve the above system of linear equations.

One commonly raised issue here is, in what case there is one and only one stationary distribution (\\(\lambda=1\\))?

### Stochastic Matrix
A [stochastic matrix](https://en.wikipedia.org/wiki/Stochastic_matrix) is a real square matrix, with each row or column summing to 1, which can be used to described a finite state Markov chain. It [can be shown](https://en.wikipedia.org/wiki/Stochastic_matrix#Definition_and_properties) that every stochastic matrix has, at least, an eigenvector associated to the eigenvalue 1 and the largest absolute value of all its eigenvalues is also 1.

Hence we know every finite state Markov chain has at least one stationary distribution.

### Closed Communicating Classes (Irreducible Closed Subsets)
Following [the definition on Wikipedia](https://en.wikipedia.org/wiki/Markov_chain#Reducibility)
- A state \\(j\\) is said to be accessible from a state \\(i\\) if a system started in state \\(i\\) has a non-zero probability of transitioning into state \\(j\\) at some point.
- A state \\(i\\) is said to communicate with state \\(j\\) if both \\(i\\) is accessible from \\(j\\) and vice versa. A communicating class is a maximal set of states \\(C\\) such that every pair of states in \\(C\\) communicates with each other.
- A communicating class is closed if the probability of leaving the class is zero.
Having defined the notion of a closed communicating class, now we can use it to find out the number of stationary distributions. 

If there’s one closed communicating class in the chain, the chain has one stationary distribution. For example in the following transition matrix, 
\\[
\begin{matrix}
    0.5 & 0 \\\
    0.5 & 1 \\\
\end{matrix}
\\]
there's one closed communicating class consisting of only one state \\(C=\\{1\\}\\), and \\(\pi=[0, 1]\\) is the unique stationary vector.

If there’s more than one closed communicating classes, then there are infinitely many stationary distributions. For example the identity matrix
\\[
\begin{matrix}
    1 & 0 \\\
    0 & 1 \\\
\end{matrix}
\\]
each of the two states itself is a closed communicating classes, in this case any probability vector is stationary \\(\pi I=\pi \\). Here's a [nice tutorial](http://wwwf.imperial.ac.uk/~ejm/M3S4/NOTES3.pdf) for reference.

For a stochastic matrix with strictly positive entries, it's obivious that the entire state space forms one closed communicating classes, hence there's a unque staionary distribution.

## PageRank
The famous PageRank algorithm is an application of the Markov chain stationary distribution. Each web page can be think of as a state, links to other pages then represent the transition probabilities to other states, and each link on a page is considered equally probable. 

Having the states and trasition probabilities defined, the processs of a person randomly clicking on links can be modeled by such Markov chain. The PageRank algorithm ranks pages according to the stationary distribution, which can be interpreted as the probability of arriving at a page through random clicking.

As one can imagine the transition matrix can be very sparse as the number of links on a page is far less then total number of pages. To ensure that there's a unqiue stationary distribution, we can add a [damping factor](https://en.wikipedia.org/wiki/PageRank#Damping_factor) to the matrix to make all entries strictly positive, as described in [this article](http://www.ams.org/publicoutreach/feature-column/fcarc-pagerank).

## Limiting Distribution
Now we know a Markov chain has a unique stationary distribution if there's one closed communicating class, can we always converge to the stationary distribution strating from any initial state? The matrix
\\[
\begin{matrix}
    0 & 1 \\\
    1 & 0 \\\
\end{matrix}
\\]
has only one eigenvector \\(\pi=[0.5, 0.5]\\) associated to the eigenvalue 1. Say a frog start at the left lily, in the next step it will definitely jump to the right lily according to the matrix, after another step, it will definitely jump to the left, actually it will alternate between two states periodically and never reaches a stationary distribution.

If a Markov chain always converge to the stationary distribution \\(\pi\\) for any initial probability \\(X_0\\), then \\(\pi\\) is called the [limiting distribution](https://www.probabilitycourse.com/chapter11/11_2_6_stationary_and_limiting_distributions.php)
\\[\pi = \lim_{n\to\infty} P(X_n|X_0). \\]


### Ergodicity

### KL Divergence
The Second Eigenvalue of the Google Matrix https://nlp.stanford.edu/pubs/secondeigenvalue.pdf

mcmc convergence https://www.youtube.com/watch?v=D8DZjLPlWd0&index=2&list=PLaNkJORnlhZmfwQITRbxXCzot3PSXlMYb
covergence time related to differential equations, adding a constant does not change eigenvectors https://www.youtube.com/watch?v=DzqE7tj7eIM
how to find eigenvalues and eigenvectors https://www.youtube.com/watch?v=lXNXrLcoerU
Stationary distributions, Irreducibility, and Aperiodicity https://www.youtube.com/watch?v=tByUQbJdt14  https://www.youtube.com/watch?v=Pce7KKeUf5w https://www.youtube.com/watch?v=daY4lgEyEPc
