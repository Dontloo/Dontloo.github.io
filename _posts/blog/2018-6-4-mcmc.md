---
layout: post
title: "Markov Chains ;]"
modified: 2018-06-14
categories: blog
excerpt:
tags: []
date: 2018-06-14
---

## Markov Chains
By Markov chains in this blog we refer to [discrete-time homogeneous Markov chains](https://en.wikipedia.org/wiki/Markov_chain#Discrete-time_Markov_chain).

**Discrete-time Markov chains** 
A discrete-time Markov chain is a sequence of random variables \\(\{X_1, X_2, X_3, ...\}\\) with the Markov property, namely that the probability of moving to the next state depends only on the present state and not on the previous states
\\[P(X_{n+1}|X_n,â€¦,X1) = P(X_{n+1}|X_n).\\]

**Homogeneous chains**
A homogeneous Markov chain is one that does not evolve in time, that is, its transition probabilities are independent of the time step \\(n\\)
\\[P(X_{n+1}|X_n) \text{ is same } \forall n \leq 0.\\]

## Water Lily Example
To illustrate we'll take a look at the water lily example from the [Bayesian Methods for Machine Learning course](https://www.coursera.org/learn/bayesian-methods-in-machine-learning) on cousera.
[lily]
There a frog in a pond that jumps between two water lilies according to some probability. As shown in the graph, if the frog is at the left (L) lily at time \\(n\\) then it has 0.7 probability to jump to the right (R) lily and 0.3 probability to stay till the next round \\(n+1\\), if the frog is at the right lily then probability for staying and jumping are even (0.5).

This process can be modeled as a Markov chain, let \\(X_n\in\\{L, R\\}\\) be the state of the frog at time \\(n\\), we have the trainsition probabilities
\\[P(X_{n+1}=L|X_n=L)=0.3, P(X_{n+1}=R|X_n=L)=0.7\\]
\\[P(X_{n+1}=L|X_n=R)=0.5, P(X_{n+1}=R|X_n=R)=0.5.\\]

If we let the frog jumps as decribed for a long enough period, it turns out the probability of the state of the frog will coverge to a fixed distribution \\(\pi\\) regardless of the starting state, specifically \\(\pi(X=L)=0.44\\), \\(\pi(X=R)=0.56\\). It is an interesting property of Markov chains and we'll spend the rest of this blog to introduce relevant concepts and applications.

## Stationary Distribution
If a distribution \\(\pi\\) remains unchanged in the Markov chain it is called a stationary distribution, formally if satisfies the condition
\\[\pi(X=i)=\sum_j\pi(X=j)P(X=i|X=j).\\]

If there's a finite numebr of possible states in the sample space, the condition can be put in matrix form as
\\[\pi=\pi P\\]
where \\(P_{ij}=P(X=i|X=j)\\). It might look similar to the definition of eigenvalues and eigenvectors
\\[Av=\lambda v\\]
In fact \\(\pi\\) is an eigenvector of \\(P^T\\) with eigenvalue \\(\lambda=1\\)
\\[(P^T-I)\pi=0\\]
and \\(\pi\\) is a vector in null space of \\(P^T-I\\) by definition. So to find the sationary distribution of a finite state Markov chain, we only need to solve the above system of linear equations.

However the problem here is, how do we know there is one and only one \\(\lambda=1\\)?





mcmc convergence https://www.youtube.com/watch?v=D8DZjLPlWd0&index=2&list=PLaNkJORnlhZmfwQITRbxXCzot3PSXlMYb
covergence time related to differential equations, adding a constant does not change eigenvectors (pagerank) https://www.youtube.com/watch?v=DzqE7tj7eIM
how to find eigenvalues and eigenvectors https://www.youtube.com/watch?v=lXNXrLcoerU
Stationary distributions, Irreducibility, and Aperiodicity https://www.youtube.com/watch?v=tByUQbJdt14  https://www.youtube.com/watch?v=Pce7KKeUf5w https://www.youtube.com/watch?v=daY4lgEyEPc
