---
layout: post
title: "Miscellaneous"
modified:
categories: blog
excerpt:
tags: []
date: 2016-10-7
---
### probabilistic distributions over the whole real line
Why many distributions over the real number line decrease towards both ends (e.g. Cauchy, Gaussian)? Why there can not be a uniform distribution over the over entire space? Because we have to make sure the PDF integrates to one.

In other words, the improper integral 
\\[\int^\infty_{-\infty}\tilde{p}(x)dx\\]
has to converge, which means, the limit of the anti-derivative of the unnormalized distribution \\(\tilde{p}\\) at (negative) infinity has to converge to a finite number. For instance, \\(\tilde{p}(x)=\frac{1}{1+x^2}\\) (Cauchy) does and \\(\tilde{p}(x)=\frac{1}{1+|x|}\\) does not.


### tailedness
The 4th moment (or any order moment) alone doesn't tell us about the tailedness of a probabilistic distribution, the combination of two different order moments (e.g. kurtosis) does.


### normalized Euclidean-like distance
Euclidean distance is one of the most commonly used metric, its squared form is \\(\\|x-y\\|^2=\\|x\\|^2+\\|y\\|^2-2x\cdot y\\). It can be generalized to \\(\\|x\\|^2+\\|y\\|^2-cx\cdot y\\), which could be more effective in certain cases (e.g. the values of \\(x\\) and \\(y\\) are non-negative).

If provided that \\(x\\) and \\(y\\) are unit (normalized) vectors, this metric is esstially equivalent (only differ in a scaling factor \\(c\\)) to the original Euclidean distance, since \\(\\|x\\|^2\\) and \\(\\|y\\|^2\\) are then equal to 1. It is aslo equivalent to the negative cosine similarity of the unnormalized vectors ([wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity#Properties)).
