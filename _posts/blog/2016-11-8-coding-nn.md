---
layout: post
title: "Notes on Coding Neural Networks"
modified: 2016-11-8
categories: blog
excerpt:
tags: []
date: 2016-11-8
---

### Existing Neural Network Libraries
There're many brilliant open source libraries for neural networks and deep learning. Some of them try to wrap every function they provide into an uniform interface or protocol (e.g. caffe and tensorflow), such well encapsulated libraries might be easy to use but difficult to change. As the rapid development of deep learning, it becomes a common need for people in the field to experiment new ideas beyond those encapsulations, often I found that the very interface I need is just the source code, and the very protocol I need is just the programming language.

[Pytorch](http://pytorch.org/) does a great job on that, before I found Keras I wrote a library myself for the purpose of *going from idea to result with the least possible delay*, followings are some implementation notes. It mostly talks about feedforward neural networks (FFNNs), but the idea should be applicable to recurrent networks as well.

### Same Network for Training and Test?
Is the same network used for both training and test? If it were in the last century, the answer might be yes. But no. 
For instance, recently developed layers such as batch normalization behave differently at training and test time, and sometimes people only need some intermediate result as feature representations for other tasks at test time. 
Here're a few examples of different architectures in training and test, [Deep Learning Face Attributes in the Wild](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liu_Deep_Learning_Face_ICCV_2015_paper.pdf), [FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648), [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114).

So network architectures can be different, computations can be different,
the only thing that connects training and test is (a subset of) the parameters. 
In terms of coding, we totally should decouple the training and test networks as two different functions,
that is to say, the training specifies how to search for the parameters, the test specifies how to use the parameters.

### Implementing a Neural Network Library
The neural network library I wrote follows the following key points, hopefully it would help sorting out the related concepts in terms of implementation.  

- The notion of a network breaks down into two types of functions, one is for optimizing an objective function w.r.t some parameters through an optimizer (training), the other computes the output for some input given the parameters (test).
- Function are defined by stacking up layers.
- A layer can have its parameter, input and output, it may behave differently for training, test or else (e.g. whether disconnect the gradient or not). 
- Parameters are defined independently can be shared between layers and functions.

### Variational Autoencoder Example

![Here](https://raw.githubusercontent.com/dontloo/dontloo.github.io/master/images/vae.png) are samples generated by a variational autoencoder implemented with my own library.
