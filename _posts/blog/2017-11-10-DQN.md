---
layout: post
title: "The Deep Q-network Continued"
modified: 2017-10-20
categories: blog
excerpt:
tags: []
date: 2017-11-10
---
[There](https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/) is a very nice introduction about the deep Q-network (DQN), 
this blog is just some complement about the network for generating the targets.

As described in the [paper](https://www.nature.com/articles/nature14236.pdf) the loss funtion is 
\\[ E[((y-Q(s,a,;\theta_i))^2]\\] 
where
\\[ y= r+\gamma max_{a'}Q(s',a',\theta_i^-).\\]

The difficuly is that the target value changes as training proceeds as \\(y\\) depends on the output of the network.
The underlying reason is that the network is not consistant with itself.

The papper addresses this by first let the targets stay unchanged, then wait for the network to converge, 

tbc
