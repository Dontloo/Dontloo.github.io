---
layout: post
title: "Conditional Random Fields"
modified:
categories: blog
excerpt:
tags: []
date: 2016-10-16
modified: 2016-10-16
---

There're many good references for Conditional Random Fields (CRFs), 
however I found them mostly about big pictures in the perspective of graphical models, 
because that's all most people need to know (in order to use a CRF library). 
This post covers a bit more the practical part.

### The Big Picture

A CRF is defined as (ref: Machine Learning: A Probabilistic Perspective) 
\\[ p(y|x,w) = \frac{1}{Z(x,w)}\prod_{c}\psi_c(y_c|x,w). \\]

Typically there are three things we'd like to do with CRFs as with [HMMs](http://jedlik.phy.bme.hu/~gerjanos/HMM/node6.html), which are  
- evaluation:  \\( p(y|x,w) \\)  
- learning:  \\( \text{arg}\max\limits_{w} p(y|x,w) \\)   (MLE), or MAP if added some regularization, or max-margin learning  
- decoding:  \\( \text{arg}\max\limits_{y} p(y|x,w) \\) subject to \\( p(y^{max}|x,w)=\max\limits_{y}p(y|x,w) \\) (i.e. to find the set of values \\( y \\) that are individually the most probable)  

### Potentials (Energy Functions)
The potentials \\(\psi_c\\) are usually defined in the log-linear form of energy functions as,
\\[ \psi_c(y_c|x,w)=\exp(-E(x,y_c))=\exp(w_c^T\phi_c(x,y_c)) \\]
where \\( w_c \\) are model parameters and \\( \phi_c(x,y_c) \\) are called the features.

For instance, to define a Gaussian like potential, we can define 
\\( \phi_c(x,y_c)=(x-y_c)^2 \\), \\( w_c=-1/(2\sigma^2) \\), so
\\[ \psi_c(y_c|x,w)=\exp(-\frac{(x-y_c)^2}{2\sigma^2}). \\]

If the hidden variables \\(y\\) are categorical, it might seem strange to make the features dependent of \\(y\\), so in such case \\(y\\) often serves as an indicator. For example a simple chain structured CRF can be written as
\\[ p(y|x)=\frac{1}{Z(x)}\exp(\sum_{j=1}^{m}\langle w_{y_j},\phi_c(x_j)\rangle + \sum_{j=1}^{m-1}T_{y_j, y_{j+1}}). \\]

Moreover, the feature function \\(\phi_c\\) can also be learned by say, a neural network.

### Dynamic Programming

The difficulty often arises in the evaluating the normalizer \\( Z(x) \\) and related gradients, for chain structured CRFs, there's a way to evaluate \\( Z(x) \\) and the corresponding gradient terms efficiently, which is known as the forward-backward algorithm. There are variant versions of the forward-backward algorithm in different contexts (e.g. for doing the E step of EM for HMMs) but the key concept is the same, which is by applying dynamic programming in both the forward and backward directions to compute the marginals.

Dynamic programming in this case can be simply thought of as the distributive rule reversed:
\\[ ab + ac = a(b + c) \\]
By taking the \\( a \\) out of the parenthesis we are converting an exponential number of computations into polynomial. Dynamic programming can also be applied to the decoding task in a similar manner as the Viterbi algorithm for HMMs.

### More
A bit more about the forward-backward algorithm for CRFs, [a question on forward-backward algorithm in CRF++](http://stats.stackexchange.com/a/240610/95569).  
A bit more about the feature functions for the log-linear model, [How can I implement a CRF feature function?](http://stats.stackexchange.com/a/248761/95569), [Log-linear models and conditional random fields](http://cseweb.ucsd.edu/~elkan/250Bwinter2012/loglinearCRFs.pdf).

