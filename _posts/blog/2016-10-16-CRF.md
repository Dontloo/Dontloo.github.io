---
layout: post
title: "Conditional Random Fields"
modified:
categories: blog
excerpt:
tags: []
date: 2016-10-16
modified: 2016-10-16
---

There're many good references for Conditional Random Fields (CRFs), 
however I found them mostly about big pictures in the perspective of graphical models, 
because that's all most people need to know (in order to use a CRF library). 
This post covers a bit more the practical part.

### The Big Picture

A CRF is defined as (ref: Machine Learning: A Probabilistic Perspective) 
\\[ p(y|x,w) = \frac{1}{Z(x,w)}\prod_{c}\psi_c(y_c|x,w). \\]

Typically there are three things we'd like to do with CRFs as with [HMMs](http://jedlik.phy.bme.hu/~gerjanos/HMM/node6.html), which are  
- evaluation:  \\( p(y|x,w) \\)  
- learning:  \\( \text{arg}\max\limits_{w} p(y|x,w) \\)   (MLE, or MAP if added some regularization)  
- decoding:  \\( \text{arg}\max\limits_{y} p(y|x,w) \\) subject to \\( p(y^{max}|x,w)=\max\limits_{y}p(y|x,w) \\) (i.e. to find the set of values \\( y \\) that are individually the most probable)  

### Potentials (Energy Functions)
The potentials \\(\psi_c\\) are usually defined in the log-linear form of energy functions as,
\\[ \psi_c(y_c|x,w)=\exp(-E(x,y_c))=\exp(w_c^T\phi_c(x,y_c)) \\]
where \\( w_c \\) are model parameters and \\( \phi_c(x,y_c) \\) are called the features.

For instance, to define a Gaussian like potential, we can define 
\\( \phi_c(x,y_c)=[x^2, xy_c, y_c^2]^T \\), \\( w_c=[-1/(2\sigma^2), 1/\sigma^2, -1/(2\sigma^2)]^T \\), so
\\[ \psi_c(y_c|x,w)=\exp(w_c^T\phi_c(x,y_c))=\exp(-\frac{x^2 - 2xy_c + y_c^2}{2\sigma^2}). \\]

If the hidden variables \\(y\\) are categorical, it might seem strange to make the features dependent of \\(y\\), so in this case \\(y\\) often serves as an indicator. For example a simple chain structured CRF can be written as
\\[ p(y|x)=\frac{1}{Z(x)}\exp(\sum_{j=1}^{m}\langle w_{y_j},\phi_c(x_j)\rangle + \sum_{j=1}^{m-1}T_{y_j, y_{j+1}}). \\]

Moreover, the feature function \\(\phi_c\\) can also be learned by say, a neural network.

### Dynamic Programming

The difficulty often arises in the evaluating the normalizer \\( Z(x) \\) and related gradients, as said in Pattern Recognition and Machine Learning

>  In the case of continuous variables, the required integrations may not have closed-form analytical solutions,
while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration. 
For discrete variables, the marginalizations involve summing over all possible configurations of the hidden variables, 
and though this is always possible in principle, we often find in practice that 
there may be exponentially many hidden states so that exact calculation is prohibitively expensive.

For chain structured CRFs, there's a way to evaluate \\( Z(x) \\) and the corresponding gradient terms efficiently, which is known as the forward-backward algorithm. There are variant versions of the forward-backward algorithm in different contexts (e.g. for doing the E step of EM for HMMs) but the key concept is the same, which is by applying dynamic programming in both the forward and backward directions to compute the marginals.

Dynamic programming in this case can be simply thought of as the distributive rule:
\\[ ab + ac = a(b + c) \\]
By taking the \\( a \\) out of the parenthesis we are converting an exponential number of computations into polynomial. Dynamic programming can also be applied to the decoding task in a similar manner as the Viterbi algorithm for HMMs.

There's a post regarding the forward-backward algorithm for CRFs in a bit more detail on StackExchange, [A question on forward-backward algorithm in CRF++](http://stats.stackexchange.com/q/208747/95569).

