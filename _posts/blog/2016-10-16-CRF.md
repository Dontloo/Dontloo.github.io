---
layout: post
title: "About Conditional Random Fields"
modified:
categories: blog
excerpt:
tags: []
date: 2016-10-13
modified: 2016-10-13
---

There're many good references for Conditional Random Fields (CRFs), 
but they are mostly about big pictures in the perspective of graphical models, 
because that's all most people need to know (in order to use a CRF library). 
This post covers a bit more the practical part.

### Intro

A CRF is defined as (ref: Machine Learning: A Probabilistic Perspective) 
\\[ p(y|x,w) = \frac{1}{Z(x,w)}\prod_{c}\psi_c(y_c|x,w). \\]
In practice it's often assumed a log-linear representation for the potentials, 
\\[ \psi_c(y_c|x,w)=\exp(w_c^T\phi_c(x,y_c)). \\]

Typically there are three things we'd like to do with CRFs as with the [HMMs](http://jedlik.phy.bme.hu/~gerjanos/HMM/node6.html), which are  
evaluation:  \\( p(y|x,w) \\)  
learning (MLE, or MAP if added some regularization):  \\( \text{arg}\max\limits_{w} p(y|x,w) \\)  
decoding:  \\( \text{arg}\max\limits_{y} p(y|x,w) \\) subject to \\( p(y^{max}|x,w)=\max\limits_{y}p(y|x,w) \\) (that is, to find the set of values \\( y \\) that are individually the most probable)

The difficulty often arises in the evaluating the normalizer \\( Z(x) \\), as said in Pattern Recognition and Machine Learning

>  In the case of continuous variables, the required integrations may not have closed-form analytical solutions,
while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration. 
For discrete variables, the marginalizations involve summing over all possible configurations of the hidden variables, 
and though this is always possible in principle, we often find in practice that 
there may be exponentially many hidden states so that exact calculation is prohibitively expensive.

### Dynamic Programming

For chain structured CRFs, there's a way to evaluate \\( Z(x) \\) and the corresponding gradient terms efficiently, 
which is by applying dynamic programming in both the forward and backward directions 
(similar idea as the forward-backward algorithm for HMM, but the forward-backward algorithm is for EM instead of gradient based optimizations).

Dynamic programming in this contex can be thought of as the distributive rule:
\\[ ab + ac = a(b + c) \\]
By taking \\( a \\) out of the perenthesis we are converting an exponential number of computations into polynomial.

### Energy Funtions (Potentials)

If the label y is one-hot vector, then its often 
