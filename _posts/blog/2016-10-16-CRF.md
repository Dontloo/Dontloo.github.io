---
layout: post
title: "About Conditional Random Fields"
modified:
categories: blog
excerpt:
tags: []
date: 2016-10-13
modified: 2016-10-13
---

There're many good references for Conditional Random Fields (CRFs), 
but they are mostly about big pictures in the perspective of graphical models, 
because that's all most people need to know (in order to use a CRF library).

This post covers a bit more the practical part.

A CRF is defined as (ref: Machine Learning: A Probabilistic Perspective) 
\\[ p(y|x,w) = \frac{1}{Z(x,w)}\prod_{c}\psi_c(y_c|x,w). \\]
In practice it's often assumed a log-linear representation for the potentials, 
\\[ \psi_c(y_c|x,w)=\exp(w_c^T\phi_c(x,y_c)). \\]

Typically there are three things we'd like to do with CRFs as with the [HMMs](http://jedlik.phy.bme.hu/~gerjanos/HMM/node6.html), which are

- evaluation: \\( p(y|x,w) \\)  
- decoding: \\( \text{arg}\max\limits_{y} p(y|x,w) \\)  
- learning (MLE, or MAP if added some regularization): \\( \text{arg}\max\limits_{w} p(y|x,w) \\)  

The difficulty often arises in the evaluating the normalizer \\( Z(x,w) \\), as said in Pattern Recognition and Machine Learning

>  In the case of continuous variables, the required integrations may not have closed-form analytical solutions,
while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration. 
For discrete variables, the marginalizations involve summing over all possible configurations of the hidden variables, 
and though this is always possible in principle, we often find in practice that 
there may be exponentially many hidden states so that exact calculation is prohibitively expensive.

tbc
train, grad
chain, dp
