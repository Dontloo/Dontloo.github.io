---
layout: post
title: "Softmax"
modified:
categories: blog
excerpt:
tags: []
date: 2017-6-18
modified: 2017-6-18
---

### Normalization functions
There are many ways of doing normalization, in this article we focus on the following type
\\[y=f(x) \text{ s.t. } y_i\geq 0, \sum y_i=1. \\]
Such normalization functions are widely used in the machine learning field, for example, to represent discrete probability distributions.

### How to construct a normalization function
An straight forward approach is
1. Map input from real numbers to non-negative numbers (to ensure \\(y_i\geq 0\\))
2. Divide by the sum (to ensure \\(\sum y_i=1\\))
  
### Softmax
Softmax is a good example of this type
1. \\(e^x\\) map \\(x\\) from real numbers to positive numbers 
2. Divide by \\(\sum e^{x_i}\\)
    
Then we have Softmax \\[y_i = \frac{e^{x_i}}{\sum e^{x_j}}\\]

### Why the name Softmax
Suppose the max function returns a one-hot vector with one at the index of maximum value and zero elsewhere, for example
\\[x=[-2,-1], \quad max(x)=[0,1].\\]
Softmax will return a "softened" version where the index of maximum value still has the largest value but is smaller than one  
\\[softmax(x)=[0.269, 0.731].\\]

### Why Softmax
AFAIK the popularity of Softmax is mainly for these two advantages when used together with the cross entropy loss.

1. The computation is fast and stable,
\\[L =\sum t_i\log y_i = \sum t_i\log\frac{e^{x_i}}{\sum e^{x_j}} = \sum t_i (x_i - \log\sum e^{x_j})\\]
The operation \\(\log\sum\e^{x}\\) can be implemented as [LogSumExp](https://en.wikipedia.org/wiki/LogSumExp) to prevent numerical under/overflow. Otherwise hand if we explicitly compute the results of \\(y_k\\), we'll often need to clip its value to avoid *Nan*s, which will also lead to inaccurate gradients.
	
2. The derivative is nice, 
\\[\frac{\partial L}{\partial x_i} = y_i - t_i.\\]
Ref: [Derivative of Softmax loss function](https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function)


Therefore in practice, some libraries implement Softmax as part of the loss function for the aforementioned benefits in the forward and backward passes. That's probably why some people call it softmax loss. The operation of computing \\(x_i - \log\sum\exp(x_i)\\) is often called the *LogSoftMax*.
