---
layout: post
title: "Softmax"
modified:
categories: blog
excerpt:
tags: []
date: 2017-6-18
modified: 2017-6-18
---

### Normalization functions
There are many normalization functions in this article we focus on such function \\(f\\)
\\[y=f(x) \text{ s.t. } y_i\geq 0, \sum y_i=1 \\]
It is widely used in the machine learning field, for example, to represent discrete probability distributions.

### How to construct a normalization function
    1. Map input from real numbers to non-negative numbers
    2. Divide by the sum
  
### Softmax
    1. \\(e^x\\) map \\(x\\) from real numbers to positive numbers 
    2. Divide by \\(\sum e^{x_i}\\)
\\[y_i = \frac{e^{x_i}}{sum e^{x_j}}\\]

### Why called softmax
Suppose the max function returns a one-hot vector with one at the index of maximum value and zero elsewhere, for example
\\[x=[2,4]  max(x)=[0,1].\\]
Softmax will return a softened version vector 
\\[softmax(x)=[1/3, 2/3].\\]

### Why softmax
When used with cross entropy loss, 
\\[L =\sum t_i\log y_i = \sum t_i\log\frac{\exp(x_i)}{\sum\exp(x_j)},\\]
	log_sum_exp can be implemented efficiently with robustness against numerical under/overflow
	
	2. the derivative is nice, 
  \\[\partial L/\partial x_i = y_i - t_i\\]
Ref: https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function

For a neural network classifier with cross-entropy loss and softmax in the output layer, it follows 
\\[\log p_\theta(t|x)=\sum t_k\log y_k = \sum t_k\log \frac{\exp(a_k)}{\sum\exp(a_j)},\\]
where \\(y_k\\) is the output of the neural network and \\(a_k\\) the activation of the output layer.
The loss function can further be derived as 
\\[\log p_\theta(t|x) = \sum t_k\log \frac{\exp(a_k)}{\sum\exp(a_j)} = \sum t_k (a_k - \log\sum\exp(a_j)),\\]
where \\(\log\sum\exp\\) can be implemented as [LogSumExp](https://en.wikipedia.org/wiki/LogSumExp) to prevent numerical under/overflow. Otherwise hand if we explicitly compute the results of \\(y_k\\), we'll often need to clip its value to avoid *Nan*s, which will also lead to inaccurate gradients.

Therefore in practice, many popular frameworks will instead make \\(a_k\\) the output of the network and move softmax into the loss function. That's probably the reason why some people call it softmax loss. The operation of computing \\(a_k - \log\sum\exp(a_j)\\) is called the *LogSoftMax*.
