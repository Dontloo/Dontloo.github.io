---
layout: post
title: "Softmax"
modified:
categories: blog
excerpt:
tags: []
date: 2017-6-18
modified: 2017-6-18
---

### Normalization functions
There are many normalization functions in this article we focus on such function \\(f\\)
\\[y=f(x) \text{ s.t. } y_i\geq 0, \sum y_i=1 \\]
It is widely used in the machine learning field, for example, to represent discrete probability distributions.

### How to construct a normalization function
    1. Map input from real numbers to non-negative numbers
    2. Divide by the sum
  
### Softmax
    1. \\(e^x\\) map \\(x\\) from real numbers to positive numbers 
    2. Divide by \\(\sum e^{x_i}\\)
\\[y_i = \frac{e^{x_i}}{sum e^{x_j}}\\]

### Why called softmax
Suppose the max function returns a one-hot vector with one at the index of maximum value and zero elsewhere, for example
\\[x=[2,4]  max(x)=[0,1].\\]
Softmax will return a softened version vector 
\\[softmax(x)=[1/3, 2/3].\\]

### Why softmax
When used with cross entropy loss, 
\\[L =\sum t_i\log y_i = \sum t_i\log\frac{\exp(x_i)}{\sum\exp(x_j)} = \sum t_i (x_i - \log\sum\exp(x_j)),\\]
	\\(\log\sum\exp\\) can be implemented as [LogSumExp](https://en.wikipedia.org/wiki/LogSumExp) to prevent numerical under/overflow. Otherwise hand if we explicitly compute the results of \\(y_k\\), we'll often need to clip its value to avoid *Nan*s, which will also lead to inaccurate gradients.
	
	2. the derivative is nice, 
  \\[\partial L/\partial x_i = y_i - t_i\\]
Ref: https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function


Therefore in practice, some implementation makes softmax as part of the loss function and makes \\(x_i\\) the output of a classifier for the aforementioned benefits in forward and backward passes. That's probably why some people call it softmax loss. The operation of computing \\(x_i - \log\sum\exp(x_i)\\) is called the *LogSoftMax*.
