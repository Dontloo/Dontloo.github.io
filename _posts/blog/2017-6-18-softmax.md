---
layout: post
title: "Softmax"
modified:
categories: blog
excerpt:
tags: []
date: 2017-6-18
modified: 2017-6-18
---

### Normalization functions
There are many ways of doing normalization in this article we focus on the following type
\\[y=f(x) \text{ s.t. } y_i\geq 0, \sum y_i=1. \\]
Such normalization functions are widely used in the machine learning field, for example, to represent discrete probability distributions.

### How to construct a normalization function
An straight forward approach is

    1. Map input from real numbers to non-negative numbers (to ensure \\(y_i\geq 0\\))
    2. Divide by the sum (to ensure \\(\sum y_i=1\\))
  
### Softmax
Softmax is a good example of this type

    1. \\(e^x\\) map \\(x\\) from real numbers to positive numbers 
    2. Divide by \\(\sum e^{x_i}\\)
    
Then we have softmax \\[y_i = \frac{e^{x_i}}{sum e^{x_j}}\\]

### Why the name softmax
Suppose the max function returns a one-hot vector with one at the index of maximum value and zero elsewhere, for example
\\[x=[-2,-1]  max(x)=[0,1].\\]
Softmax will return a "softened" version where the index of maximum value still has the largest value but is smaller than one  
\\[softmax(x)=[0.269, 0.731].\\]

### Why softmax
AFAIK the popularity of softmax is mainly for these two advantages when used together with cross entropy loss.

    1. The computation is fast and stable,
    \\[L =\sum t_i\log y_i = \sum t_i\log\frac{\exp(x_i)}{\sum\exp(x_j)} = \sum t_i (x_i - \log\sum\exp(x_j))\\]
    \\(\log\sum\exp\\) can be implemented as [LogSumExp](https://en.wikipedia.org/wiki/LogSumExp) to prevent numerical under/overflow. Otherwise hand if we explicitly compute the results of \\(y_k\\), we'll often need to clip its value to avoid *Nan*s, which will also lead to inaccurate gradients.
	
    2. the derivative is nice, 
    \\[\frac{\partial L}{\partial x_i} = y_i - t_i.\\]
    Ref: https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function


Therefore in practice, some implementation makes softmax as part of the loss function and makes \\(x_i\\) the output of a classifier for the aforementioned benefits in forward and backward passes. That's probably why some people call it softmax loss. The operation of computing \\(x_i - \log\sum\exp(x_i)\\) is called the *LogSoftMax*.
