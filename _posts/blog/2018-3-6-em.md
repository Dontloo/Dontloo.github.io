Because we can not observe the true value of latent variables, we estimate a distribution $q_i(z_i)$ as `completions` (of the missing value $z$) for each data point, then optimize the sum of expectations $$\sum_z E_{z_i}[\log \frac{p(x,z|\theta)}{q(z_i)}]$$ as an lower bound (Jensen's inequality) of the data likelihood $p(X|\theta)$.


There's a distribution in the LDA paper: http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf  
[![enter image description here][1]][1]
[![enter image description here][2]][2]

My understanding is, $p(w|\theta,\beta)$ is actually a multinomial distribution over words, say $p(w|\theta,\beta) = Multi(w|\mu)$, where $\mu$ the event probabilities depends on $\theta$, $\mu=f_\beta(\theta)$. 

The plotted distribution is a distribution of $\mu$ given the Dirichlet prior $p(\theta|\alpha)$, and it's acquired by applying a [change of variable to the prior pdf][3] $p(\mu|\alpha,\beta) = p(f_\beta(\theta)|\alpha)$.

Usually $\mu$ is of greater dimensionality than $\theta$, so the pdf $p(\mu|\alpha,\beta)$ only lives on a subspace of the simplex since $f_\beta$ is a mapping from a lower dimensional space to a higher one.
