---
layout: post
title: "Expectation Maximization Sketch"
modified:
categories: blog
excerpt:
tags: []
date: 2018-3-6
modified: 2018-3-6
---

### Intro
Say we have observed data \\(X\\), the latent variable \\(Z\\) and parameter \\(\theta\\), we want to maximize the log-likelihood \\(\log p(X\|\theta)\\).  Sometimes it's not an easy task, probably because it doesn't have a closed-from solution, the gradient is difficult to compute, or there're complicated constraints that \\(\theta\\) must satisfy. 

If somehow the joint log-likelihood \\(\log p(X, Z\|\theta)\\) can be maximized more easily, we can turn to the Expectation Maximization algorithm for help. There're serveral ways to formulate the EM algorithm, as will be discussed in this blog.

### Joint Log-Likelihood
The basic idea is just to optimize the joint log-likelihood \\(\log p(X, Z\|\theta)\\) instead of the data log-likelihood \\(\log p(X\|\theta)\\). But since the true values of latent variables \\(Z\\) are unknown, we need to estimate a posterior distribution \\(p(z|x, \theta)\\) for each data point \\(x\\), then **maximize** the **expected** log-likelihood over the posterior  
\\[\sum_x E_{p_x}[\log p(x,z|\theta)].\\]  

The optimization follows two iterative steps. The E-step computes the expectation under the current parameter \\(\theta'\\)
\\[\sum_x \sum_{z} p(z|x, \theta') \log p(x,z|\theta) = Q(\theta|\theta').\\]
The M-step tries to find the new parameter \\(\theta\\) that maximizes \\(Q(\theta|\theta')\\). It turns out that such method is gauranteed to find a local maximum data log-likelihood \\(\log p(X\|\theta)\\), as will be shown in later sections.

### Evidence Lower Bound (ELBO)
One dirivation of EM is via constructing the evidence lower bound of \log p(X\|\theta) using Jensen's inequality
\\[\log p(X\|\theta) = \sum_x \log p(x\|\theta)\\]
\\[= \sum_x \log \sum_z p(x, z\|\theta)\\]
\\[= \sum_x \log \sum_z q_x(z) \frac{p(x, z\|\theta)}{q_x(z)}\\]
\\[\geq \sum_x \sum_z q_x(z) \log \frac{p(x, z\|\theta)}{q_x(z)}\\]
where \\(q_x(z)\\) is an arbitrary distribution over the latent varibale associated with data point \\(x\\).

At the E-step, we find the \\(q\\) that maximize this lower bound while keeping \\(\theta\\) fixed. Since \\(q\\) has to satisfy the properties of being a probability distribution, the problem becomes, for each data point \\(x^{(i)}\\),
\\[\max_{q_i(z^{(i)})} \sum_z^{(i)} q(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}\|\theta)}{q(z^{(i)})}\\]  
s.t.  
\\[q(z^{(i)})\geq 0, \sum_z q(z^{(i)}) = 1.\\]
It can be shown the solution to this is \\(q_i(z^{(i)}) = p(z|x^{(i)}, \theta)\\), which is accordance with our previous discussion. Specifically, if \\(Z\\) is discrete, it can be solved using Lagrange multipliers, see [this tutorial by Justin Domke](https://www.ics.uci.edu/~smyth/courses/cs274/readings/domke_notes_on_EM.pdf) (my teacher ;).

At the M-step we keep \\(q\\) fixed and maximize over \\(\theta\\).
\\[\sum_Z q(Z) \log \frac{p(X, Z\|\theta)}{q(Z)} \\]
\\[= \sum_Z q(Z) \log p(X, Z\|\theta) - \sum_Z q(Z) q(Z) \\]
\\[= Q(\theta|\theta') + H(q) \\]
The second term \\(H(q)\\) is independent of \\(\theta\\) given \\(q\\) is fixed, so the M-step is also in line with the previous formulation.

At the M-step we maximize the lower bound w.r.t \\(\theta^{(t)}\\)) and at the E-step we set a new lower bound that is tight at \\(\theta^{(t)}\\)).

### Latent Distribution
Let's see now to decompose the lower bound from the data likelihood without using the inequality.
\\[\log p(X\|\theta) = \sum_Z q(Z) \log p(X\|\theta) \\]
\\[= \sum_Z q(Z) \log \frac{p(X,Z\|\theta)}{p(Z\|X,\theta)} \\]
\\[= \sum_Z q(Z) \log \frac{p(X,Z\|\theta)q(Z)}{p(Z\|X,\theta)q(Z)} \\]
\\[= \sum_Z q(Z) \log \frac{p(X,Z\|\theta)}{q(Z)} - \sum_Z q(Z) \log \frac{p(Z\|X,\theta)}{q(Z)}\\]
\\[= F(q, \theta) + D_{KL}(q(Z) \| p(Z\|X,\theta))\\]
We see \\(F(q, \theta)\\) is the lower bound and the remaining term is the KL divegence between the latent distribution \\(q\\) and the posterior \\(p(Z\|X,\theta)\\). 

So in the E-step \\(F(q, \theta)\\) is maximized w.r.t \\(q\\) while holding \\(\theta\\) fixed. Since \\(\log p(X\|\theta)\\) is an upper bound of \\(F(q, \theta)\\) that does not depend on \\(q\\), the lagest value of \\(F(q, \theta)\\) occurs when \\(D_{KL}(q(Z) \| p(Z\|X,\theta))=0\\), we have again \\(q(Z) = p(Z\|X,\theta)\\). In the M-step we optimize \\(F(q, \theta)\\) w.r.t \\(\theta\\), which is the same as the discussed above.

### KL Divergence
Note that \\(F(q, \theta)\\) above is also in the form of KL divergence.
\\[\sum_Z q(Z) \log \frac{p(X,Z\|\theta)}{q(Z)} = \sum_i \sum_{z{(i)}} q_i(z{(i)}) \log \frac{p(x{(i)},z{(i)}\|\theta)}{q_i(z{(i)})} \\]
If we let \\(q(x,z) = q(z\|x)p(x)\\), where \\(q(z{(i)}\|x{(i)}) = q_i(z{(i)})\\) and \\(p(x)=\frac{1}{\|X\|}\sum_i\delta(x{(i)})\\) is a distribution that place all its mass on the observed data \\(X \\), then we have
\\[\int_x p(x)f(x) dx = \|X\| sum_i f(x^{(i)}). \\]
Then the formula can be rewritten as 
\\[\frac{\|X\|}{\log\|X\|} \int_x \sum_z q(z{(i)},x{(i)}) \log \frac{p(x,z\|\theta)}{ q(z{(i)}\|x{(i)}) } dx  = -\frac{\|X\|}{\log\|X\|} D_{KL}(q(x,z) \| p(x,z\|\theta)). \\]
Therefore the E-step is minimizing \\(D_{KL}(q(x,z) \| p(x,z\|\theta))\\) w.r.t \\(p_\theta\\).

Similarly for the \\(D_{KL}(q(Z) \| p(Z\|X,\theta))\\) term, it can be rewritten as 
\\[- \sum_Z q(Z) \log \frac{p(Z\|X,\theta)}{q(Z)} = - \sum_i \sum_z q_i(z{(i)}) \log \frac{p(z\|x,\theta)}{ q_i(z{(i)}) }\\]
\\[= - \|X\| \int_x \sum_z q(x,z) \log \frac{p(x, z, \theta)}{q(x,z)} dx = \|X\| D_{KL}(q(x,z) \| p(x,z\|\theta)) \\]
Then the M-step becomes is minimizing the same KL divergence \\(D_{KL}(q(x,z) \| p(x,z\|\theta))\\) but w.r.t \\(q\\). 

Since \\(q(x,z)\\) follows the restriction that it must aline with the data, while \\(p(x,z\|\theta)\\) must be a distribution under the specified model, they can be thought of as living on two manifolds in the space of all distributions, the data manifold and the model manifold respectively. EM can be viewed as to minimize the distance between two manifolds \\(D_{KL}(q(x,z) \| p(x,z\|\theta))\\). More about the geometric view of EM please refer to [this paper](http://mi.eng.cam.ac.uk/~wjb31/PUBS/igmlc.ciss96.pdf).

### Log-sum to Sum-log
In spite of these different views of EM, the advantage of EM lies in Jensen's inequality
\\[\log \sum_Z q(Z) \frac{p(X, Z\|\theta)}{q(Z)} \geq \sum_Z q(Z) \log \frac{p(X, Z\|\theta)}{q(Z)}\\]
By moving the logarithm inside the summation, if the joint distibution \\(p(X, Z\|\theta)\\) belongs to the exponentail family, it turns a log-sum-exp operation into a weighted summation of the exponents (often sufficient statistics), which could be easier to optimize.

### Alternatives for E-step and M-step
Sometimes we aren't able to rich the optimal solution to the E-step or the M-step, probably because the difficulty in calculation, optimization, trade-off between simplicity and accuracy, or other restrictions on the distributions or paramters. In these cases, alternative approaches can be used.

For example K-means is a special case of EM for GMMs, where the latent distributioin is restricted to be hard assignments. In LDA, a prior distribution was added to the paramter, thus made the paramter another latent variable, and the posterior distribition of latent varibales becomes difficult to compute. Variational methods were used for approximation, specifically, the latent distribution \\(q\\) is characterized by a variational model with paramter \\(\psi\\). Then in the E-step we optimize w.r.t \\(\phi\\) and in the M-step we optimize w.r.t \\(\theta\\). In this setting some parameters still can not be optimized analytically, gradient based methods are used.
