---
layout: post
title: "Expectation Maximization"
modified:
categories: blog
excerpt:
tags: []
date: 2018-3-6
modified: 2018-3-6
---

### Intro
Say we have data \\(X\\), the latent variable \\(Z\\) and parameter \\(\theta\\), we want to maximize the log-likelihood \\(\log p(X\|\theta)\\). The situation is the log-likelihood is difficult to optimize using a gradient based method, probably because the gradient is hard to compute, or there're complicated constraints that \\(\theta\\) must satisfy. While somehow the joint log-likelihood \\(\log p(X, Z\|\theta)\\) can be easily optimized, and that's where the Expectation Maximization comes in.

There're serveral ways to formulate the EM algorithm, which are discussed as follows.

### Joint Log-Likelihood
The basic idea is just to optimize the joint log-likelihood \\(\log p(X, Z\|\theta)\\) instead of the data log-likelihood \\(\log p(X\|\theta)\\). Because we can not observe the true value of latent variables, we estimate the conditional distribution \\(p_i(z^{(i)}|x^{(i)}, \theta)\\) for each data point \\(x^{(i)}\\), then **maximize** the **expected** log-likelihood over \\(q_i(z_i)\\)  
\\[\sum_i E_{p_i}[\log p(x^{(i)},z^{(i)}|\theta).\\]  

The optimization follows two iterative steps. At the E-step compute the expectation under the current parameter \\(\theta'\\)
\\[\sum_i \sum_{z^{(i)}} p(z^{(i)}|x^{(i)}, \theta') \log p(x^{(i)},z^{(i)}|\theta) dz^{(i)} = Q(\theta|\theta').\\]
M-step we find the new parameter \\(\theta\\) that maximize \\(Q(\theta|\theta')\\).

It turns out that such method is gauranteed to find a local maximum data log-likelihood \\(\log p(X\|\theta)\\), as shown in the folloing sections.

### Lower Bound
We next show how to dirive EM using Jensen's inequality.
\\[\log p(X\|\theta) = \log \sum_Z p(X, Z\|\theta) \\]
\\[= \log \sum_Z q(Z) \frac{p(X, Z\|\theta)}{q(Z)} \\]
\\[\geq \sum_Z q(Z) \log \frac{p(X, Z\|\theta)}{q(Z)}\\]

Hence \\(F(q, \theta)\\) is a lower bound of the data log-likelihood \\(\log p(X\|\theta)\\log p(X\|\theta)\). 


At the E-step, we find the \\(q\\) that maximize this lower bound while keeping \\(\theta\\) fixed. Since \\(q\\) has to satisfy the properties of being a probability distribution, the problem becomes, for each data point \\(x^{(i)}\\),
\\[\max_{q_i(z^{(i)})} \sum_z^{(i)} q(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}\|\theta)}{q(z^{(i)})}\\]  
s.t.  
\\[q(z^{(i)})\geq 0, \sum_z q(z^{(i)}) = 1.\\]
It can be shown the solution to this is \\(q_i(z^{(i)}) = p(z|x^{(i)}, \theta)\\), which is accordance with our previous discussion. Specifically, if \\(Z\\) is discrete, it can be solved using Lagrange multipliers, see this tutorial by [Justin Domke](https://www.ics.uci.edu/~smyth/courses/cs274/readings/domke_notes_on_EM.pdf) (my teacher ;).

At the M-step we keep \\(q\\) fixed and maximize over \\(\theta\\).
\\[F(q, \theta) = \sum_Z q(Z) \log \frac{p(X, Z\|\theta)}{q(Z)} \\]
\\[= \sum_Z q(Z) \log p(X, Z\|\theta) - \sum_Z q(Z) q(Z) \\]
\\[= Q(\theta|\theta') + H(q) \\]
The second term \\(H(q)\\) is independent of \\(\theta\\) given \\(q\\) is fixed, so the M-step is also in line with the previous formulation.

At the M-step we maximize the lower bound and at the E-step we set a new lower bound that is tight at \\(\theta^{(t)}\\)).

### Latent Distribution

### Information Geometry

### Log-sum Sum-log

### Gibbs Sampling

### Alternatives for E-step and M-step


