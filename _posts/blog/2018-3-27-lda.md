---
layout: post
title: "Summary of Latent Dirichlet Allocation"
modified:
categories: blog
excerpt:
tags: []
date: 2018-3-27
modified: 2018-3-27
---

### What does LDA do?  
The LDA model converts a bag-of-words document in to a (sparse) vector, where each dimension corresponds to a topic.
Topics are learned to capture statistical relations of words, here's a nice illustration from [Bayesian Methods for Machine Learning
by National Research University Higher School of Economics](https://www.coursera.org/learn/bayesian-methods-in-machine-learning/home/welcome).
![lda](https://raw.githubusercontent.com/dontloo/dontloo.github.io/master/images/lda1.png)  

### LDA vs. vector space model  
The vector space model representation of a document is a normalized vector in the "word" space, 
while the LDA representation is a normalized vector in the "topic" space.
By converting a bag-of-words document from a "word" space into a "topic" space, we incorporate word correlations learned under the topics.

### History
The [LDA paper](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) gives a good summary of the history of text modeling.  
**Unigram model**  
\\[p(\mathbf{w})=\prod_{n=1}^Np(w_n) \\]  
**Mixture of unigrams**  (all words from one topic)  
\\[p(\mathbf{w})=\sum_zp(z)\prod_{n=1}^Np(w_n) \\]  
**Probabilistic latent semantic indexing**  (each word from one topic)  
\\[p(d,w_n)=p(d)\sum_zp(w_n|z)p(z|d)\\]  
**Latent Dirichlet allocation**  (latent Dirichlet prior)  
\\[p(\mathbf{w},\alpha,\beta)=\int p(\theta|\alpha)\prod_{n=1}^N\sum_{z_n}p(w_n|z_n,\beta)p(z_n|\theta) d\theta \\]

### Dirichlet Distribution
Multinomial distribution takes the form
\\[Mult(\mathbf{n}|\mathbf{p}, N)={N\choose \mathbf{n}}\prod_{k=1}^Kp_k^n_k}\\]  
it is a distribution over the exponents. Dirichlet distribution is in a similar form, only it's a distribution over the bases
\\[Dir(\mathbf{p}|\mathbf{\alpha})=\frac{}{\mathbf{B}(\mathbf{\alpha})}\mathbf{n}\prod_{k=1}^Kp_k^a_k\\]  
Dirichlet distribution is **sparse** and **multimodal** when \\(\alpha\\) is less than one as illustrated [here](https://cs.stanford.edu/~ppasupat/a9online/1080.html).

### Latent Dirichlet
A document is a multinomial over topics, a topic is a multinomial over words, the distribution of words given a document \\(p(w|\theta,\beta) = \sum_zp(z|\theta)p(w|z,\beta)=Multi(w|\mu)\\), where \\(\mu=f_\beta(\theta)=\theta\beta\\) is a vector in the "word" space.  

As shown in the LDA paper, the distribution of \\(\mu\\) is continuous mixture with weights from the Dirichlet prior \\(p(\theta|\alpha)\\), which can also be acquired by applying a change of variable to the prior pdf \\(p(\mu|\alpha,\beta) = p(f_\beta(\theta)|\alpha)\\).  
![lda](https://raw.githubusercontent.com/dontloo/dontloo.github.io/master/images/lda2.png)  

The pdf $p(\mu|\alpha,\beta)$ actually lives on a subspace of the simplex when $\mu$ is of greater dimensions than $\theta$, and exhibits a multimodal structure when \\(\alpha<1\\).





