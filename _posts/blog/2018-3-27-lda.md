---
layout: post
title: "Summary of Latent Dirichlet Allocation"
modified:
categories: blog
excerpt:
tags: []
date: 2018-3-27
modified: 2018-3-27
---

### What does LDA do?  
The LDA model converts a bag-of-words document in to a (sparse) vector, where each dimension corresponds to a topic.
Topics are learned to capture statistical relations of words, here's a nice illustration from [Bayesian Methods for Machine Learning
by National Research University Higher School of Economics](https://www.coursera.org/learn/bayesian-methods-in-machine-learning/home/welcome).
![lda](https://raw.githubusercontent.com/dontloo/dontloo.github.io/master/images/lda1.png)  

### LDA vs. vector space model  
The vector space model representation of a document is a normalized vector in the "word" space, 
while the LDA representation is a normalized vector in the "topic" space.
By converting a bag-of-words document from a "word" space into a "topic" space, we incorporate word correlations learned under the topics.

### History
The [LDA paper](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) gives a good summary of the history of text modeling.  
**Unigram model**  
\\[p(\mathbf{w})=\prod_{n=1}^Np(w_n) \\]  
**Mixture of unigrams**  (all words from one topic)  
\\[p(\mathbf{w})=\sum_zp(z)\prod_{n=1}^Np(w_n) \\]  
**Probabilistic latent semantic indexing**  (each word from one topic)  
\\[p(d,w_n)=p(d)\sum_zp(w_n|z)p(z|d)\\]  
**Latent Dirichlet allocation**  (latent Dirichlet prior)  
\\[p(\mathbf{w},\alpha,\beta)=\int p(\theta|\alpha)\prod_{n=1}^N\sum_{z_n}p(w_n|z_n,\beta)p(z_n|\theta) d\theta \\]

### Dirichlet Distribution
Multinomial distribution takes the form
\\[Mult(\mathbf{n}|\mathbf{p}, N)={N\choose \mathbf{n}}\prod_{k=1}^Kp_k^{n_k}\\]  
it is a distribution over the exponents. Dirichlet distribution is in a similar form, only it's a distribution over the bases
\\[Dir(\mathbf{p}|\mathbf{\alpha})=\frac{1}{\mathbf{B}(\mathbf{\alpha})}\prod_{k=1}^Kp_k^{a_k-1}\\]  
Dirichlet distribution is **sparse** and **multimodal** when \\(\alpha\\) is less than one as illustrated [here](https://cs.stanford.edu/~ppasupat/a9online/1080.html).

### Latent Dirichlet
A document is a multinomial over topics, a topic is a multinomial over words, the distribution of words given a document \\(p(w|\theta,\beta) = \sum_zp(z|\theta)p(w|z,\beta)=Multi(w|\mu)\\), where \\(\mu=f_\beta(\theta)=\theta\beta\\) is a vector in the "word" space.  

As shown in the LDA paper, the distribution of \\(\mu\\) is continuous mixture with weights from the Dirichlet prior \\(p(\theta|\alpha)\\), which exhibits a multimodal structure when \\(\alpha<1\\). It can also be acquired by applying a change of variable to the prior pdf \\(p(\mu|\alpha,\beta) = p(f_\beta(\theta)|\alpha)\\).  

When \\(\mu\\) is of greater dimensions than \\(\theta\\) it actually lives on a subspace of the simplex, where the bases are the rows of \\(\beta\\) (topics).
![lda](https://raw.githubusercontent.com/dontloo/dontloo.github.io/master/images/lda2.png)

### Training and Inference
**Variational inference + expectation-maximiztion**  
In the LDA model described above, \\(\theta\\) and \\(z\\) are latent variables, \\(\alpha\\) and \\(\beta\\) are parameters.  

This paper
[David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. 2003.](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) shows how to use expectation-maximiztion (EM) for a maximum likelihood estimation (MLE) of the parameters \\(\alpha\\) and \\(\beta\\). The main problem is, the distribution of latent variables \\(p(\theta,\mathbf{z}|\mathbf{w},\alpha,\beta)\\) is intractable to compute, a nice approximation mehtod based on variational inference was given in the paper.  

Since MLE looks for high likelihood, there's an inclination towards sparse \\(\alpha\\) and \\(\beta\\).

**Collapsed Gibbs sampling**  
This paper [Thomas L Griffiths and Mark Steyvers. Finding scientific topics. 2004.](http://www.pnas.org/content/101/suppl_1/5228.short) present an extention of the original LDA model, which is to model \\(\beta\\) as a latent variable with another Dirichlet distribution \\(p(\phi)\\) as its prior. Then by treating \\(\alpha\\) and \\(\phi\\) as hyper-parameters, the paper uses collapsed Gibbs sampling to sample from the distribution \\(p(\mathbf{z}|\mathbf{w})\\).

For inference, collapsed Gibbs sampling can also be used to sample the latent variables   
\\[p(\tilde{\mathbf{z}}|\tilde{\mathbf{w}},\mathbf{z},\mathbf{w}),\\] then \\(\beta\\) can be estimated by   
\\[\beta=E_{p(\mathbf{z}|\mathbf{w})}[p(\beta|\mathbf{w},\mathbf{z})]\approx p(\beta|\mathbf{w},\bar{\mathbf{z}}).\\]

### Others
[Similarity between documents in LDA “word” vectors space?](https://stats.stackexchange.com/questions/337193/similarity-between-documents-in-lda-word-vectors-space)



