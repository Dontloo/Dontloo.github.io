---
layout: post
title: "About Latent Dirichlet Allocation"
modified:
categories: blog
excerpt:
tags: []
date: 2018-3-27
modified: 2018-3-27
---

### What does LDA do?  
The LDA model converts a bag-of-words document in to a (sparse) vector, where each dimension corresponds to a topic.
Topics are learned to capture statistical relations of words, here's a nice illustration from [Bayesian Methods for Machine Learning
by National Research University Higher School of Economics](https://www.coursera.org/learn/bayesian-methods-in-machine-learning/home/welcome).
![lda](https://raw.githubusercontent.com/dontloo/dontloo.github.io/master/images/lda1.png)  

### LDA vs. vector space model  
The vector space model representation of a document is a normalized vector in the "word" space, 
while the LDA representation is a normalized vector in the "topic" space.
By converting a bag-of-words document from a "word" space into a "topic" space, we incorporate word correlations learned under the topics.

### History
The [LDA paper](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) gives a good summary of the history of text modeling.  
##Unigram model##  
\\[p(\mathbf{w})=\prod_{n=1}^Np(w_n) \\]

