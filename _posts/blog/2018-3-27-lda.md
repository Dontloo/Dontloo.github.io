---
layout: post
title: "Summary of Latent Dirichlet Allocation"
modified:
categories: blog
excerpt:
tags: []
date: 2018-3-27
modified: 2018-3-27
---

### What does LDA do?  
The LDA model converts a bag-of-words document in to a (sparse) vector, where each dimension corresponds to a topic.
Topics are learned to capture statistical relations of words, here's a nice illustration from [Bayesian Methods for Machine Learning
by National Research University Higher School of Economics](https://www.coursera.org/learn/bayesian-methods-in-machine-learning/home/welcome).
![lda](https://raw.githubusercontent.com/dontloo/dontloo.github.io/master/images/lda1.png)  

### LDA vs. vector space model  
The vector space model representation of a document is a normalized vector in the "word" space, 
while the LDA representation is a normalized vector in the "topic" space.
By converting a bag-of-words document from a "word" space into a "topic" space, we incorporate word correlations learned under the topics.

### History
The [LDA paper](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) gives a good summary of the history of text modeling.  
**Unigram model**  
\\[p(\mathbf{w})=\prod_{n=1}^Np(w_n) \\]  
**Mixture of unigrams**  (all words from one topic)  
\\[p(\mathbf{w})=\sum_zp(z)\prod_{n=1}^Np(w_n) \\]  
**Probabilistic latent semantic indexing**  (each word from one topic)  
\\[p(d,w_n)=p(d)\sum_zp(w_n|z)\p(z|d)\\]  
**Latent Dirichlet allocation**  (latent Dirichlet prior)  
\\[p(\mathbf{w},\alpha,\beta)=\int p(\theta|\alpha)\prod_{n=1}^N\sum_{z_n}p(w_n|z_n,\beta)p(z_n|\theta) d\theta \\]

### Dirichlet Distribution
Multinomial distribution takes the form
\\[Mult(\mathbf{n}|\mathbf{p}, N)=\binom{N}{\mathbf{n}\prod_{k=1}^Kp_k^n_k}\\],  
it is a distribution over the exponents. Dirichlet distribution is in a similar form, only it's a distribution over the bases
\\[Dir(\mathbf{p}|\mathbf{\alpha})=\frac{}{\mathbf{B}(\mathbf(\alpha))}{\mathbf{n}\prod_{k=1}^Kp_k^a_k}\\].  


